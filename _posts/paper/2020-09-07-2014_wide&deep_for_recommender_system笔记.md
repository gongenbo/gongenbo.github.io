---
layout: post
title: DLRS2016 Wide & Deep Learning for Recommender System笔记
categories: [paper]
description: DLRS2016 Wide & Deep Learning for Recommender System笔记
keywords: DLRS,wide,deep,Facebook,predict,rank,笔记,翻译
---

# 论文

论文原址：[Wide & Deep Learning for Recommender Systems](https://arxiv.org/abs/1606.07792)

# 摘要
Wide & Deep 模型的核心思想是结合线性模型的记忆能力和 DNN 模型的泛化能力，从而提升整体模型性能。Wide & Deep 已成功应用到了 Google Play 的app推荐业务，并于TensorFlow中封装。该结构被提出后即引起热捧，在业界影响力非常大，很多公司纷纷仿照该结构并成功应用于自身的推荐等相关业务。

**DLRS** 全称 Workshop on Deep Learning for Recommender Systems，2016举办第一届，旨在加速深度学习在RecSys社区中的传播和应用。深度学习已经在很多领域上取得巨大成功，会议组织相信它有潜力成为下一代推荐系统的核心，现在已经是将它们应用于RecSys的时候了。

# 1. 介绍

推荐系统可以被看作为搜索排名系统，其中输出是一组用户和上下文的信息，用户和上下文信息，输出是每一项的排名列表。给定查询，建议任务是查找数据库中的相关项目，然后根据特定目标（如点击或购买）对项目进行排名。
 
推荐系统的一个挑战，和一般的搜索排名系统类似，是实现两者 **memorization** 和 **generalization**。

**Memorization** 可以粗略地定义为学习项目或特征的频繁共发生，并利用（**exploiting**）历史数据中可用的关联。基于 Memorization 的推荐通常更热门，并且与用户已经执行的操作的项目直接相关。

**Generalization** 基于相关性的传递性，并探索（**explores**）了新的特征组合，过去从未或很少发生。与 Memorization 相比，Generalization 倾向于提高推荐项目的多样性。

1.1 广义线性模型(generalized linear models)

对于在产业化环境中的大规模的在线推荐和排名系统，广义线性模型（如逻辑回归）被广泛使用，因为它们简单、可扩展且可解释。

模型通常使用 one-hot encoding 在二进制的稀疏特征上进行训练。

例如： 用户安装 Netflix ，则二进制功能 user_installed_app_netflix 具有值 1。Memorization 可以通过跨产品对稀疏的特征如 AND(user_installed_app=netflix, impression_app=pandora”) 的转换来实现。这个解释了如何通过目标标签对匹配同时出现的特征值。

1.2 广义化 Generalization

广义化 Generalization 可以通过粒度较低的特征值来添加，例如 AND(user_installed_**category**=video, impression_category=music)，但是通常需要手动的特征工程。跨产品转换的一个限制是它们不概括到未出现在训练数据中的查询项的特征对。

1.3 嵌入式的模型(Embedding-based models)

嵌入式的模型，例如 分解机、深度学习网络，可以通过学习每个查询和项特征的低维密集嵌入向量来概括到以前看不到的查询项特征对，从而减特征值处理的负担。

但是，当基础查询项矩阵稀疏且排名较高（例如具有特定偏好的用户或吸引力较小的合适项目）时，很难学习查询和项的有效低维表示形式。在这种情况下，大多数查询项对之间不应有交互，但密集嵌入将导致对所有查询项对进行非零预测，从而可以过度概括和提出不太相关的建议。另一方面，具有跨产品特征转换的线性模型可以记住这些” 异常规则”，而参数要少得多。

![Alt text](/img/paper/wide_deep/figure1.png)
本论文通过如下所示的 Figure 1（该论文主要的贡献） 联合训练线性模型组件和神经网络组件，在一个wide & deep学习框架中通过一个模型实现 memorization 记忆化 和 generalization 广泛化 。

该文件的主要贡献包括：

- wide & deep 学习框架，用于共同训练线性模型和具有稀疏输入的通用推荐系统（generic recommender systems）的特征转换。
Wide＆Deep 推荐系统的实施和评估是在 Google Play（一家拥有超过 10 亿活跃用户和超过 100 万个应用程序的移动应用程序商店）上生产的。
我们在 TensorFlow 中将我们的实现以及高级 API 开源了
具有嵌入和的前馈神经网络虽然想法很简单，但我们证明 Wide＆Deep 框架显着提高了移动应用商店中的应用获取率，同时满足了培训和服务速度方面的要求。


# 2. 推荐系统总览
![Alt text](/img/paper/wide_deep/figure2.png)
应用程序推荐系统的概述如 Figure 2 所示。当用户访问应用程序商店时，将生成一个查询，其中可能包含各种用户和上下文功能。 推荐系统返回一个应用程序列表（也称为展示次数），用户可以在其上执行某些操作，例如点击或购买。 这些用户操作以及查询和印象将作为学习者的训练数据记录在日志中。

由于数据库中有超过一百万个应用程序，因此在服务等待时间要求（通常为 10毫秒）内为每个查询详尽地为每个应用程序评分是很困难的。 因此，接收到查询的第一步是检索。 检索系统使用各种信号（通常是机器学习的模型和人工定义的规则的组合）返回简短匹配的项目清单。 减少候选库后，排名系统将所有项目按其得分进行排名。 分数通常为 P（y | x），带有特征 x 的用户动作的概率，包括用户功能（例如，国家 / 地区，语言，人口统计信息），上下文功能（例如，设备，一天中的小时，星期几）以及 印象功能（例如，应用程序年龄，应用程序的历史统计信息）。 在本文中，我们重点研究使用wide & deep学习框架的排名模型。

# 3. wide & deep学习


## 3.1 wide 组件

The wide component 是形式 = wTx + b 的广义线性模型，如图 1（左）所示。y是预测，x = [x1，x2，…，xd] 是特征的向量，w = [w1， w2，…，wd] 是模型参数，并且是偏差。特征集包括原始输入特征和变换特征。 最重要的转换之一是跨产品转换，其定义为：

![Alt text](/img/paper/wide_deep/formula1.png)

其中 cki 是一个布尔变量，如果第 i 个特征是第 k 个变换 φk 的一部分，则为 1，否则为 0。对于二进制特征，请进行叉积变换（例如，“AND（gender = female，language = en ）”）仅当构成要素（“性别 = 女性” 和 “语言 = zh-CN”）全部为 1 时为 1，否则为 0。 这捕获了二元特征之间的相互作用，并为广义线性模型增加了非线性。


## 3.2 deep 组件
The deep component 是前馈神经网络，如图 1（右）所示，对于分类要素，原始输入是要素字符串（例如，“language = en”）。 首先将这些稀疏的高维分类特征转换为低维且密集的实值向量，通常将其称为嵌入向量。 嵌入的维度通常约为 O（10）到 O（100）。 随机初始化嵌入向量，然后训练值以最小化模型训练期间的 finalloss 函数。 然后将这些低维密集嵌入向量馈入神经网络的隐藏层中。 具体而言，每个隐藏层执行以下计算：

![Alt text](/img/paper/wide_deep/formula2.png)                                    

其中层号和激活函数通常为线性整流单元（ReLU）。a（l），b（l）和 W（l）是第 1 层的激活，偏差和模型权重。 

## 3.3 wide & deep 联合训练

The wide component and deep component 使用其输出对数比值的加权和作为预测进行组合，然后将其馈入(fed to)一种常见的逻辑损失函数以进行联合训练。请注意，联合训练和综合之间是有区别的。联合的时候，单独模型在不相互了解的情况下进行单独训练，并且它们的预测仅在推理时组合，而在训练时不组合。相比之下，联合训练通过在训练时同时考虑最宽和最深的部分以及其总和的权重来同时优化所有参数。模型大小也会产生影响：对于整体而言，由于训练是不相交的，因此通常每个个体模型的大小都需要更大（例如具有更多特征和变换），以使整体工作达到合理的准确性。相比之下，对于联合训练，仅需使用少量跨产品功能变换来补充较深部分的弱点，而不是完整尺寸的较宽模型。

通过使用最小批量随机优化，同时从输出向模型的宽和深部分同时传播梯度，可以完成宽深模型的优化训练。 在实验中，我们使用具有 L1 规则化的跟随规则化领导（FTRL）算法 [3] 作为模型大部分的优化器，并使用 AdaGrad [1] 作为模型的最佳部分。组合模型如图 1 所示（中心）.

对于逻辑回归问题，模型的预测为：

![Alt text](/img/paper/wide_deep/formula3.png)  

其中 Y 是二进制类标签，σ（・）是 S 形函数，φ（x）是原始特征 x 的叉积变换，，并且 b 是偏差项。wwide 是所有模型权重的向量，和 wdeep 是施加在最终激活 a（lf）上的权重。


# 4. 系统实现

![Alt text](/img/paper/wide_deep/figure3.png)

应用程序推荐传递途径的实现包括三个阶段：数据生成，模型训练和模型服务，如 Figure 3 所示。

## 4.1 数据生成

在此阶段，一段时间内的用户和应用曝光数据将用于生成训练数据。 每个示例对应一个曝光。 标签为 app acquisition：如果此程序被安装，则为 1；否则为 0。

在此阶段还会生成词汇表，这些表是将分类功能字符串映射为整数 ID 的表。系统会为所有出现最少次数的字符串特征计算 ID 空间。 通过将特征值 x 映射到其累积分布函数 P（X≤x）（除以 intonqquantiles），可以将连续的实值特征标准化为 [0,1]。 第 i 个分位数中的值的归一化值。 在数据生成期间计算分位数边界。

![Alt text](/img/paper/wide_deep/figure4.png)


## 4.2 模型训练

我们在实验中使用的模型结构如图 4 所示。在训练期间，我们的输入层接收训练数据和词汇，并生成稀疏和密集特征以及标签。 广泛的组件包括用户安装的应用程序和展示应用程序的跨产品转换。 对于模型的深层部分，将为每个分类特征学习 32 维嵌入向量。 我们将所有嵌入与密集特征连接在一起，从而得到大约 1200 维的密集向量。 然后将级联的向量输入 3 个 ReLU 层，最后输入逻辑输出单元。

wide 和 deep 联合模型已针对超过 5,000 亿个示例进行了训练。 每次收到一组新的训练数据时，都需要对模型进行重新训练。 但是，每次从头开始的重新训练在计算上都是昂贵的，并且会延迟从数据到达到提供更新模型的时间。 为了应对这一挑战，我们实施了热启动系统，该系统使用嵌入的模型和先前模型的线性模型权重来初始化新模型。

在将模型加载到模型服务器之前，需要对模型进行空运行以确保它不会在提供实时流量方面引起问题。 我们根据以前的模型经验性地验证模型质量，以进行完整性检查。

## 4.3 模型服务

对模型进行训练和验证后，我们会将其加载到模型服务器中。 对于每个请求，服务器从应用程序检索系统和用户功能部件接收一组应用程序候选者，以对每个应用程序进行评分。 然后，从最高得分到最低得分对应用程序进行排名，然后按顺序将这些应用程序显示给用户。 通过在 Wide＆Deep 模型上运行前向推理过程来计算分数。

为了满足 10 毫秒量级的每个请求，我们通过多线程并行运行来优化性能，方法是并行运行较小的批处理，而不是在单个批处理推理步骤中对所有候选应用程序评分。

# 5. 应用获取App Acquisitions
为了评估现实世界推荐系统中的广泛学习和深度学习的有效性，我们进行了现场实验，并从两个方面评估了该系统：应用程序获取和服务性能。

我们在 A / B 测试框架中进行了 3 周的在线实时实验。 对于对照组，随机选择了 1％的用户，并向其提供由先前版本的排名模型生成的推荐，该排名模型是高度优化的全范围逻辑回归模型，具有丰富的跨产品特征变换。 对于实验组，向 1％的用户展示了由 Wide＆Deep 模型产生的建议，并接受了相同的功能集培训。 如表 1 所示，

![Alt text](/img/paper/wide_deep/table1.png)

Wide＆Deep 模型相对于对照组（具有统计学意义）将应用商店主登陆页面上的应用获取率提高了 + 3.9％。 还仅使用具有相同特征和神经网络结构的模型的深层部分，将结果与另一个 1％的小组进行了比较，宽和深层模式在仅深层模型之上具有 + 1％的增益（具有统计意义） 。

除了在线实验，我们还显示了接收器操作员区域下的特征
脱机设置的保持线上的曲线（AUC）。 虽然 Wide＆Deep 的离线 AUC 略高，但对在线流量的影响更大。 一种可能的原因是，脱机数据集中的印象和标签是固定的，而在线系统可以通过将概括与记忆相结合来生成新的探索性建议，并从新的用户响应中学习。

## 5.1 服务表现

商业移动应用商店面临高流量，以高吞吐量和低延迟提供服务是一项挑战。 在流量高峰时，我们的推荐服务器每秒可记录超过 1000 万个应用程序。 使用单线程时，对单个批处理中的所有候选者评分需要 31 毫秒。 我们实施了多线程，并将每个批处理分成较小的大小，这将客户端延迟显着减少到 14 ms（包括服务开销），如表 2 所示。

![Alt text](/img/paper/wide_deep/table2.png)

# 6. 相关工作
将宽线性模型与叉积特征变换以及具有密集嵌入的深层神经网络相结合的想法受到了以前的工作的启发，例如因式分解机 [5]，该因式分解机通过将两个变量之间的相互作用作为两个点之间的乘积进行因式分解，从而对线性模型进行了泛化。 低维嵌入向量。 在本文中，我们通过神经网络而非点积学习嵌入之间的高度非线性相互作用，从而扩展了模型的功能。

在语言模型中，已提出通过 * n * -gram 特征联合训练递归神经网络（RNN）和最大熵模型，以通过学习输入和输出之间的直接权重来显着降低 RNN 的复杂性（例如，隐藏层大小）[4]。 ]。 在计算机视觉中，深度残差学习 [2] 已被用来减少训练更深层模型的难度并通过跳过一层或多层的快捷连接提高准确性。 神经网络与图形模型的联合训练也已应用于图像的人体姿势估计 [6]。 在这项工作中，我们探索了前馈神经网络的联合训练

线性模型，在稀疏特征和输出单元之间具有直接连接，用于稀疏输入数据的通用推荐和排名问题。

在推荐系统文献中，已经通过将内容信息的深度学习与评级矩阵的协作过滤（CF）耦合来探索协作深度学习 [7]。 以前在移动应用推荐系统上也有过工作，例如 AppJoy，它在用户的应用使用记录中使用 CF [8]。 与先前工作中基于 CF 或基于内容的方法不同，我们联合针对应用推荐系统的用户和展示数据训练了 Wide＆Deep 模型。

记忆和概括对于推荐系统都很重要。 宽线性模型（Wide linear models）可以使用跨积特征变换有效地记住稀疏特征交互，而深度神经网络可以通过低维嵌入将其推广到以前看不见的特征交互。 我们提出了广泛和深度学习框架，以结合两种类型的模型的优势。 我们在大型商业应用商店 Google Play 的推荐系统上制作并评估了该框架。 在线实验结果表明，与 “仅宽” 和 “仅深” 模型相比，“宽和深” 模型导致了应用获取方面的显着改善。

# 总结

## Memorization
之前大规模稀疏输入的处理是：通过线性模型 + 特征交叉。通过特征交叉能够带来很好的效果并且可解释性强。但是 Generalization（泛化能力）需要更多的人工特征工程。
## Generalization
相比之下，DNN 几乎不需要特征工程。通过对低纬度的 dense embedding 进行组合可以学习到更深层次的隐藏特征。但是，缺点是有点 over-generalize（过度泛化）。推荐系统中表现为：会给用户推荐不是那么相关的物品，尤其是 user-item 矩阵比较稀疏并且是 high-rank（高秩矩阵）

# 代码
[TensorFlow官方实现代码](https://github.com/tensorflow/models/tree/master/official/wide_deep)

# 参考
- [「Wide & Deep Learning for Recommender Systems」- 论文阅读](https://learnku.com/articles/40434)