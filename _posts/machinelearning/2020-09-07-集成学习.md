---
layout: post
title: 集成学习介绍
categories: [machinelearning]
description: 集成学习介绍
keywords: 集成学习,boosting,bagging,Xgboost,GBDT
---

# 1. 介绍

本文主要总结机器学习中常用的集成方法：boosting，bagging，stacking。在滴滴交易引擎实习面试中回答的不好，利用本文总结一下。

# 3. Boosting

**串行** ，各个基分类器之间有依赖，对前一层分错的样本给与更高的**权重**
 
*AdaBoost，GBDT，XGBoost，LightGBM* 都属于Boosting思想。

先从训练集中用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前的弱学习器1学习误差率高的训练样本点的权重变高，使得这些样本点在后面的弱学习器中能够得到足够的重视，在调整权重后重新训练弱学习期2，如此重复进行，直到弱学习器达到事先指定的数目t，最后将这t个弱学习器通过集合策略进行整合，得到最终的强学习器。
boosting的思想用如下公式表示：

$$F_m(x)=f_0+\alpha_1f_1(x)+...+\alpha_mf_m(x)$$

## adaboost
AdaBoost的核心是修改样本的权重，让学习器更看重权重大的样本


## GBDT 与 XGBoost区别

1. **基学习器:** 传统的 GBDT 以 CART 树作为基学习器，XGBoost 还支持线性分类器，这个时候XGBoost相当于 L1 和 L2 正则化的 Logistics Regression（分类）或者 Linear Regression（回归）
2. **导数信息:** 传统的 GBDT 在优化的时候只用到一阶导数信息，XGBoost 则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数
3. **正则项:** XGBoost 在代价函数中加入了正则项，用于控制模型的复杂度。从权衡方差偏差来看，它降低了模型的方差，使学习出来的模型更加简单，防止过拟合，这也是 XGBoost 优于传统 GBDT 的一个特性
4. **列抽样:** XGBoost 借鉴了随机森林的做法，支持列抽样，不仅防止过拟合，还能减少计算
5. **缺失值处理:** 对于特征的值有缺失的样本，XGBoost 可以自动学习出它的分裂方向
6. **并行:** XGBoost的并行不是 tree 粒度的并行，XGBoost 也是一次迭代完才能进行下一次迭代的。XGBoost 的并行是在`特征粒度`上的。决策树的学习最耗时的一个步骤就是对特征的值进行排序——要确定最佳分割点，XGBoost 在训练之前，预先对数据进行了排序，然后保存为 block 结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个 block 结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行


# 4. Bagging

**并行** ，每个个体单独判断，再用**投票**的方式做出最后决策

*随机森林（Random Forest）* 属于bagging思想。

bagging原理就是给定T个弱学习器，假设有m个训练样本，用来训练这T个弱学习器的样本是从这m个训练样本中有放回的选取m个样本(这就是bootstrap采样)进行训练，最后对这T个弱学习器采取结合策略来合并，对于弱学习器，一般使用决策树。对于合并多个弱学习器采用的方法：如果是分类问题，通常采用简单的投票法，得到最多票数的类别或者之一作为最终模型的输出；对于回归问题，通常使用简单的平均法，对T个弱学习器的得到的回归结果进行算术平均作为最终模型的输出。
讲一下bootstrap采样：因为是有放回的采样，所以之前采样到的样本放回后有可能继续被采样到，这里每个样本被采样到的概率是1m，不被采样到的概率是1−1m。那么m次采样都没有被采样到的概率是(1−1m)m，这里如果m趋近与无穷大，这个结果趋近于1e=0.368，即bagging每轮随机采样的过程中，训练数据集中大约有36.8%的数据没有被采样到，这些数据被称为袋外数据，由于这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。
这里讲到由于Bagging算法每次都进行采样来训练模型，因此泛化能力很强，对降低模型的方差有作用，而对模型的拟合程度会差，即模型的偏差会大一些


# 参考


- [机器学习之集成学习](https://moluchase.github.io/2018/08/30/ml03/)
- [buaawht 决策树模型](https://buaawht.github.io/2019/03/27/%E5%86%B3%E7%AD%96%E6%A0%91%E6%A8%A1%E5%9E%8B/#11-id3%E5%86%B3%E7%AD%96%E6%A0%91%E7%94%9F%E6%88%90%E7%AE%97%E6%B3%95)

- [百面机器学习 集成学习](https://zdkswd.github.io/2019/02/23/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/)