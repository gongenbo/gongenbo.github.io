---
layout: post
title: 集成学习之树模型介绍
categories: [machinelearning]
description: 集成学习之树模型介绍
keywords: 树模型,集成学习,boosting,bagging
---

# 1. 介绍

本文主要总结机器学习中常用的集成方法：boosting，bagging，stacking。在滴滴交易引擎实习面试中回答的不好，利用本文总结一下。

# 2. 决策树DT

决策树是一个树结构（可以是二叉树或非二叉树），适用于分类和回归，其方法是把特征空间划分成一系列矩阵，然后给每个矩阵赋值一个常数。
决策树的关键步骤是分裂属性，即在某个节点处按照某一特征属性的不同划分成不同的分支，其目标是让各个分裂子集尽可能的纯净(属于同一类别)；其关键是选择属性的度量准则
训练数据时，通过损失函数最小化原则构建决策树模型
决策树学习通常包括3个步骤：特征选择，决策树生成，决策树修剪
优点：模型具有可读性，分类速度快
缺点：容易过拟合，学习决策树是NP难题，数据集不平衡容易导致树模型产生偏差
实际使用技巧：

- 对于拥有大量特征的数据决策树会出现过拟合的现象。获得一个合适的样本比例的特征十分重要，因为在高维空间只有少量的样本的树是十分容易过拟合的。

- 考虑事先进行降维，使树更好的找到具有分辨性的特征

- 为防止决策树偏向主导类，可以通过从每个类中抽取相等数量的样本来进行类平衡

  ## 2.1 信息熵

  熵表示随机变量不确定性的度量
  设X是一个有限状态的离散型随机变量，其概率分布为:

  P(X=xi)=pi, i=1,2,⋯,nP(X=xi)=pi, i=1,2,⋯,n

  则随机变量X的熵定义为

  H(X)=−∑i=1npilog(pi)H(X)=−∑i=1npilog(pi)

  $H(Y|X) = \sum_{i=1}^{n}p_i H(Y|X=x_i)$

  $$H(Y|X) = \sum_{i=1}^{n}p_i H(Y|X=x_i)$$

  熵越大，则随机变量的不确定性越大。

  条件熵
  随机变量X给定的条件下，随机变量Y的条件熵H(Y|X)定义为：

  H(Y|X)=∑i=1npiH(Y|X=xi)H(Y|X)=∑i=1npiH(Y|X=xi)

  其中，pi=P(X=xi)pi=P(X=xi)。

  信息增益
  信息增益表示的是：得知特征X的信息而使得类Y的信息的不确定性减少的程度。
  具体定义如下：
  特征A对训练数据集D的信息增益g(D,A)定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即

  g(D,A)=H(D)−H(D|A)g(D,A)=H(D)−H(D|A)

  一般地，熵H(Y)与条件熵H(Y|X)之差称为互信息(mutual information).

# 3. Boosting

 **串行** ，各个基分类器之间有依赖，对前一层分错的样本给与更高的**权重**

先从训练集中用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前的弱学习器1学习误差率高的训练样本点的权重变高，使得这些样本点在后面的弱学习器中能够得到足够的重视，在调整权重后重新训练弱学习期2，如此重复进行，直到弱学习器达到事先指定的数目t，最后将这t个弱学习器通过集合策略进行整合，得到最终的强学习器。
boosting的思想用如下公式表示：



*AdaBoost，GBDT，XGBoost，LightGBM* 都属于Boosting思想。

# 4. Bagging

**并行** ，每个个体单独判断，再用**投票**的方式做出最后决策

*随机森林（Random Forest）* 属于bagging思想。



f(x)=∑m=1McmI(x∈Rm)



# 对比

## GBDT 与 XGBoost区别

1. **基学习器:** 传统的 GBDT 以 CART 树作为基学习器，XGBoost 还支持线性分类器，这个时候XGBoost相当于 L1 和 L2 正则化的 Logistics Regression（分类）或者 Linear Regression（回归）
2. **导数信息:** 传统的 GBDT 在优化的时候只用到一阶导数信息，XGBoost 则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数
3. **正则项:** XGBoost 在代价函数中加入了正则项，用于控制模型的复杂度。从权衡方差偏差来看，它降低了模型的方差，使学习出来的模型更加简单，防止过拟合，这也是 XGBoost 优于传统 GBDT 的一个特性
4. **列抽样:** XGBoost 借鉴了随机森林的做法，支持列抽样，不仅防止过拟合，还能减少计算
5. **缺失值处理:** 对于特征的值有缺失的样本，XGBoost 可以自动学习出它的分裂方向
6. **并行:** XGBoost的并行不是 tree 粒度的并行，XGBoost 也是一次迭代完才能进行下一次迭代的。XGBoost 的并行是在`特征粒度`上的。决策树的学习最耗时的一个步骤就是对特征的值进行排序——要确定最佳分割点，XGBoost 在训练之前，预先对数据进行了排序，然后保存为 block 结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个 block 结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行



# 参考


- [机器学习之集成学习](https://moluchase.github.io/2018/08/30/ml03/)
- [buaawht 决策树模型](https://buaawht.github.io/2019/03/27/%E5%86%B3%E7%AD%96%E6%A0%91%E6%A8%A1%E5%9E%8B/#11-id3%E5%86%B3%E7%AD%96%E6%A0%91%E7%94%9F%E6%88%90%E7%AE%97%E6%B3%95)

- [百面机器学习 集成学习](https://zdkswd.github.io/2019/02/23/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/)